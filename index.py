import streamlit as st
import openai
import os
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

# You can use other name defined in the .env file
key_name = 'OPENAI_API_KEY'
openai.api_key  = os.getenv(key_name)

# Prompts
_input_text = None

# The `_inferring_prompt` variable is a string that prompts the user to determine the sentiment of a
# given text. The text is represented by the variable `_txt_inferring`, which is enclosed in triple
# backticks.
def get_inferring_prompt(input_text):
    return  f"""
                What is the sentiment of the following text,
                which is delimited with triple backticks?

                text: '''{input_text}'''
                """

# The `_emotions_prompt` variable is a string that prompts the user to identify a list of emotions
# that the writer of a given text is expressing. The text is represented by the variable
# `_txt_inferring`, which is enclosed in triple backticks. The user is instructed to format their
# response as a list of enumerated items with a new line format for every item.
_emotions_prompt = f"""
                    Identify a list of emotions that the writer of the 
                    following text is expressing. Include no more than 
                    five items in the list. 

                    Format your response as a list of enumerated items with new line format for every item

                    text: '''{_input_text}'''
                    """

# Functions
def get_completion(prompt, model="gpt-3.5-turbo-16k", temperature=0):
    """
    This function uses OpenAI's chat completion API to generate a response to a given prompt using a
    specified model and temperature.
    
    :param prompt: The text prompt or message that you want to send to the OpenAI chatbot for completion
    :param model: The name of the OpenAI language model being used for text generation. In this case, it
    is set to "gpt-3.5-turbo-16k", defaults to gpt-3.5-turbo-16k (optional)
    :param temperature: The temperature parameter controls the degree of randomness or creativity in the
    model's output. A higher temperature value will result in more diverse and unexpected responses,
    while a lower temperature value will result in more conservative and predictable responses. The
    default value is 0, which means the model will always output the most likely, defaults to 0
    (optional)
    :return: The function `get_completion` returns a string, which is the response generated by the
    OpenAI chatbot model based on the given prompt.
    """
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

def check_openai(key,model="gpt-3.5-turbo-16k",max_tokens=5, temperature=0):
    """
    The function checks if an OpenAI API key is valid by attempting to create a completion with
    specified parameters.
    
    :param key: The OpenAI API key that is required to access the OpenAI API
    :param model: The OpenAI language model to use for text generation. In this case, the default model
    is "gpt-3.5-turbo-16k", defaults to gpt-3.5-turbo-16k (optional)
    :param max_tokens: The maximum number of tokens (words or symbols) that the OpenAI API will generate
    in response to a prompt, defaults to 5 (optional)
    :param temperature: Temperature is a parameter used in OpenAI's language models to control the
    creativity and randomness of the generated text. It determines how much the model should deviate
    from the most likely next word when generating text. A higher temperature value will result in more
    creative and diverse output, while a lower temperature value will, defaults to 0 (optional)
    :return: a boolean value. If the OpenAI API key is valid and the API call is successful, it will
    return True. If there is an error with the API call, it will return False and display the error
    message in the sidebar.
    """
    openai.api_key = key
    test_messages = [{"role": "user", "content": "Test"}]
    try:
        openai.ChatCompletion.create(
            model=model,
            messages=test_messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return True
    except Exception as e:
        return False

st.header("Sentiment Analysis")
st.divider()
_input_text = st.text_area('Write something : ', '')

if st.button('Inferring sentiments'):
    st.divider()
    try:
        with st.spinner('Wait for it...'):
            response = get_completion(get_inferring_prompt(_input_text), temperature=0)
            
        st.success('Done!')
        st.write(response)
        
    except Exception as e:
        st.error(f'This is an error : {e}', icon="ðŸš¨")